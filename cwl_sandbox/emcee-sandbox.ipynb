{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emcee GP Study\n",
    "\n",
    "An emcee sampler notebook, testing various things about the emcee package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook \n",
    "#restart the kernel if switching from inline to notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import corner\n",
    "\n",
    "import numpy as np\n",
    "import numpy.linalg\n",
    "import scipy.stats\n",
    "from scipy.signal import argrelextrema\n",
    "import pandas as pd\n",
    "\n",
    "import emcee\n",
    "import george\n",
    "from emcee import PTSampler\n",
    "\n",
    "from subsample import subsample # daniela's code\n",
    "from emcee_utils import walker_params, plot_gpfit\n",
    "from plotting import plot_lightcurve, plot_folded_lightcurve, plot_mcmc_sampling_results, plot_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(params):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculated the log of the prior values, given parameter values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list\n",
    "        List of all kernel parameters\n",
    "\n",
    "    param[0] : float\n",
    "        mean (between 0 and 2)\n",
    "\n",
    "    param[1] : float\n",
    "        log amplitude (between -10 and 10)\n",
    "\n",
    "    param[2] : float\n",
    "        gamma (log gamma between 0.1 and )\n",
    "\n",
    "    param[3] : float\n",
    "        log period (period between 1h and 24hrs)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sum_log_prior : int\n",
    "        sum of all log priors (-inf if a parameter is out of range)\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    p_mean = scipy.stats.uniform(0,20).logpdf(params[0])\n",
    "    p_log_amp = scipy.stats.uniform(-10,30).logpdf(params[1])\n",
    "    p_log_gamma = scipy.stats.uniform(np.log(0.1), (np.log(40)-np.log(0.1))).logpdf(np.log(params[2]))\n",
    "    p_period = scipy.stats.uniform(np.log(0.5/24), -np.log(0.5/24)).logpdf((params[3]))\n",
    "\n",
    "    sum_log_prior =  p_mean + p_log_amp + p_log_gamma + p_period\n",
    "\n",
    "    if np.isnan(sum_log_prior) == True:\n",
    "        return -np.inf\n",
    "\n",
    "    return sum_log_prior\n",
    "\n",
    "def logl(params, gp, tsample, fsample, flux_err):\n",
    "     # compute lnlikelihood based on given parameters\n",
    "     gp.set_parameter_vector(params)\n",
    "     gp.compute(tsample, flux_err)\n",
    "\n",
    "     lnlike = gp.lnlikelihood(fsample)\n",
    "\n",
    "     return lnlike\n",
    "\n",
    "def post_lnlikelihood(params):\n",
    "\n",
    "    \"\"\"\n",
    "    Calculates the posterior likelihood from the log prior and\n",
    "    log likelihood.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list\n",
    "        List of all kernel parameters\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ln_likelihood : float\n",
    "        The posterior, unless the posterior is infinite, in which case,\n",
    "        -1e25 will be returned instead.\n",
    "\n",
    "    \"\"\"\n",
    "    # calculate the log_prior\n",
    "    log_prior = prior(params)\n",
    "\n",
    "    # return -inf if parameters are outside the priors\n",
    "    if np.isneginf(log_prior) == True:\n",
    "        return -np.inf\n",
    "\n",
    "    # compute lnlikelihood based on given parameters\n",
    "    lnlike = logl(params, gp, tsample, fsample, flux_err)\n",
    "\n",
    "    try:\n",
    "        gp.compute(tsample, flux_err)\n",
    "        ln_likelihood = gp.lnlikelihood(fsample)+log_prior\n",
    "    except np.linalg.LinAlgError:        \n",
    "        ln_likelihood = -1e25\n",
    "\n",
    "    return ln_likelihood if np.isfinite(ln_likelihood) else -1e25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to extract the time stamps and flux from any data files we might have. For simulated data, we have to sample it at a candence that would be similar to real observations, meaning we have 3 types of flux and time data.\n",
    "1. __data.time & data.flux__ : all the time and flux data from the simulated data. This might span multiple days-months and has a data point for every 30 seconds using DAMIT generated data\n",
    "\n",
    "2. __time & flux__ : the time and flux data for a smaller range of dates than all_time and all_flux. This is essentially the observing window in which we are working with and time is what we will be projecting our gp fits onto\n",
    "\n",
    "3. __tsample & fsample__ : the time and flux data sampled from the time and flux data. The sampling of this is more realistic (every 10 minutes instead of every 0.5 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe929362e10>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asteroid = '1291'\n",
    "\n",
    "true_log_p = {'3200':-1.896021, '1291':-1.45813055, \n",
    "              '221':-0.8321219, '1388':-0.69789175}\n",
    "true_p = {'3200':3.603957, '1291':5.58410, \n",
    "              '221':10.443, '1388':11.9432}\n",
    "\n",
    "txt = '../data/'+str(asteroid)+'_lc_49627_to_49787.txt'\n",
    "\n",
    "data = pd.read_csv(txt, delimiter=' ',\n",
    "                 header=None, names=['time','flux'], dtype={'time':float, 'flux':float})\n",
    "\n",
    "days, delay = 5, 50\n",
    "\n",
    "# convert days to points\n",
    "span = 2880 * days\n",
    "start_pt = 2880 * delay\n",
    "\n",
    "time = np.array(data.time[start_pt:span+start_pt])\n",
    "flux = np.array(data.flux[start_pt:span+start_pt])\n",
    "\n",
    "flux_err = np.ones_like(flux) * np.std(flux)/10.0\n",
    "tsample, fsample, flux_err = subsample(time, flux, flux_err=flux_err, npoints=100, kind=\"telescope\")\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,4))\n",
    "#ax.plot(time, flux, '-', alpha=0.5, label=\"Original : \" + str(round(true_log_p[asteroid], 5)))\n",
    "ax.set_title(\"%i nights, %i data points\"%(days, len(fsample)))\n",
    "ax.set_xlabel(\"Days (JD)\")\n",
    "ax.errorbar(tsample, fsample, yerr=flux_err, fmt=\"o\", markersize=5,\n",
    "            color=\"black\", zorder=10, label=\"Sample : \" + str(len(tsample)))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data  = pd.read_csv(\"../data/221_lc_49627_to_49787.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what a Lomb-Scargle periodogram predicts the period should be based on our data so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from lombscargle import make_lsp\n",
    "from astropy.stats import LombScargle\n",
    "\n",
    "freq, power = make_lsp(tsample, fsample, flux_err, p_max=5.0)\n",
    "\n",
    "best_freq = freq[np.argmax(power)]\n",
    "best_period = 1./best_freq\n",
    "best_log_period = np.log(1./best_freq)\n",
    "\n",
    "fig, (bx,cx,dx) = plt.subplots(1,3, figsize=(9,2.5))\n",
    "fig.set_tight_layout('tight')\n",
    "bx.plot(freq, power)\n",
    "bx.set_xlabel('Frequency')\n",
    "bx.set_ylabel('Power')\n",
    "bx.vlines(best_freq, 0, 1, colors='orange', linestyles='--', \n",
    "          label = 'Best freq : ' + str(round(best_freq, 5)))\n",
    "bx.legend()\n",
    "\n",
    "cx.plot((1./freq),power)\n",
    "cx.set_xlabel('Period')\n",
    "cx.vlines(best_period, 0, 1, colors='orange', linestyles='--', \n",
    "          label = 'Best period : ' + str(round(1./best_freq, 5)))\n",
    "cx.set_xlim([0,1])\n",
    "cx.legend()\n",
    "\n",
    "dx.plot(np.log(1./freq),power)\n",
    "dx.set_xlabel('Log Period')\n",
    "dx.vlines(np.log(1./best_freq), 0, 1, colors='orange', linestyles='--', \n",
    "          label = 'Best log period : ' + str(round(np.log(1./best_freq), 5)))\n",
    "dx.set_xlim([-3.3,0])\n",
    "dx.legend()\n",
    "\n",
    "y_fit = LombScargle(tsample, fsample, flux_err).model(time, best_freq)\n",
    "#ax.plot(time, y_fit, label = \"Lomb-Scargle fit : \" + str(round(best_log_period, 5)))\n",
    "#ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the indices of local power maxima  \n",
    "best_idx = argrelextrema(power, np.greater)\n",
    "\n",
    "# sort these indices based on actual power value\n",
    "# reverse list so max is read first\n",
    "indices = np.argsort(power[best_idx[0]])[::-1]\n",
    "\n",
    "# sort our original indices based on the new \n",
    "# power-sorted indices\n",
    "best_idx = (best_idx[0]).T[indices]\n",
    "best_freqs = freq[best_idx].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe925f0d470>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_freq = best_freqs[0]\n",
    "new_period = 1./new_freq\n",
    "new_log_period = np.log(1./new_freq)\n",
    "y_fit = LombScargle(tsample, fsample, flux_err).model(time, new_freq)\n",
    "#ax.plot(time, y_fit, '--', label = \"Lomb-Scargle fit : \" + str(round(new_log_period, 5)))\n",
    "#ax.legend()\n",
    "\n",
    "bx.vlines(new_freq, 0, 1, linestyles='--', alpha=0.5,\n",
    "          label = 'New fit : ' + str(round(new_freq, 5)))\n",
    "bx.legend()\n",
    "\n",
    "cx.vlines(1./new_freq, 0, 1, linestyles='--',  alpha=0.5,\n",
    "          label = 'New period : ' + str(round(1./new_freq, 5)))\n",
    "cx.legend()\n",
    "dx.vlines(new_log_period, 0, 1, linestyles='--',  alpha=0.5,\n",
    "          label = 'New log period : ' + str(round(new_log_period, 5)))\n",
    "dx.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fe925efa908>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_folded_lightcurve(tsample, fsample, new_period)# , true_lightcurve=[time,flux])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many walkers do we want? So far there are 4 parameters/dimensions we want to study: mean, log_amp, gamma, and log_period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim, nwalkers = 4, 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp : 0.6192530987072306\n",
      "params : [ mean, log_amp, gamma, log_period]\n",
      "params : [1.0058487945897792, -0.47924120665711273, 1, -2.1487430075219596]\n",
      "cov matrix : \n",
      "[[0.01005849 0.         0.         0.        ]\n",
      " [0.         0.00479241 0.         0.        ]\n",
      " [0.         0.         0.01       0.        ]\n",
      " [0.         0.         0.         0.02148743]]\n"
     ]
    }
   ],
   "source": [
    "# initialize walker parameters\n",
    "best_log_amp = np.log(fsample.max()-fsample.min())\n",
    "params = [np.mean(fsample), best_log_amp, 1, best_log_period]\n",
    "p0, gp = walker_params(params, fsample, flux_err, nwalkers, cov_scale=1)\n",
    "#plot_gpfit(time, fsample, flux_err, gp, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, post_lnlikelihood, threads=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note : If your sampler fails within the first couple of seconds, try making the cov_scale smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/christina/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: RuntimeWarning: invalid value encountered in log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.7 s, sys: 11.7 ms, total: 12.7 s\n",
      "Wall time: 12.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mcmc_sampling = sampler.run_mcmc(p0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_steps(sampler, dims=None, p0=None, data_pts=None):\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(7,6))\n",
    "    fig.subplots_adjust(wspace=0.25, hspace=0.3)\n",
    "\n",
    "    fig.suptitle(\"Data points: \" + str(data_pts) + \"\\nMean acceptance fraction: {0:.3f}\".format(np.mean(sampler.acceptance_fraction)))\n",
    "\n",
    "    axs = [ax[0,0], ax[0,1], ax[1,0], ax[1,1]]\n",
    "\n",
    "    x = np.arange(sampler.iterations)\n",
    "\n",
    "    for i in range(sampler.dim):\n",
    "        axs[i].set_xlabel('Step Number')\n",
    "        axs[i].set_ylabel('{}'.format(dims[i]))\n",
    "\n",
    "        for j in range(len(sampler.chain)):\n",
    "            param = sampler.chain[j,:,i]\n",
    "            axs[i].plot(x, param, 'k-', alpha=0.3)\n",
    "            # fit might guess period is time range of sampling\n",
    "\n",
    "        flatchain = sampler.flatchain[:,i]\n",
    "\n",
    "\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_steps(sampler, dims = ['mean', 'log_amp', 'gamma', 'log_period'], p0=[params], data_pts=len(fsample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, fx = plt.subplots(1,1)\n",
    "\n",
    "x = np.arange(sampler.iterations)\n",
    "for i in np.arange(100):\n",
    "    fx.plot(x, sampler.lnprobability[i,:], 'k', alpha=0.3)\n",
    "fx.set_xlabel(\"Steps\")\n",
    "fx.set_ylabel(\"Ln Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampler.chain[chain, step, dimension]\n",
    "end_period = sampler.chain[:,-1,-1]\n",
    "end_period.sort()\n",
    "\n",
    "fig, fx = plt.subplots(1,1)\n",
    "fx.hist(end_period)\n",
    "fx.set_xlabel('log_period')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_chain('test_chain2', sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = read_chain('test_chain2', (100,50,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chain(file_name, sampler):\n",
    "    header = str(sampler.chain.shape)\n",
    "    np.savetxt(file_name, sampler.flatchain, header=header)\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_chain(file_name, shape):\n",
    "    new_data = np.genfromtxt(file_name, delimiter=' ')\n",
    "    # data shape can be found as the first commented line in the txt file\n",
    "    new_data = new_data.reshape(shape)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can save our data for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or read it out if we want to compare. If you want to plot the saved data, make sure to indicate it when plotting by setting __from_saved__ to True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_chain('test', (100,100,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_steps(test, dims = ['mean', 'log_amp', 'gamma', 'log_period'], p0=p0, data_pts=len(fsample), from_saved=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cgp.plot_hist(sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_mcmc_sampling_results(tsample, fsample, flux_err, gp, sampler, namestr='test', \n",
    "                           true_lightcurve = [time, flux], true_period=true_p[asteroid])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misc stuff down here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip the following cell if you are working with simulated data.\n",
    "\n",
    "Here's an alternative way of doing things if you're working with real data measurements. Since we're working with real data, we are essentially given __tsample__ and __fsample__. So we don't need to sample any points but this also means we don't know what the real light curve looks like. We can still generate a __time__ since it just needs to span from the first observation of __tsample__ to the last."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '../data/asteroid_csv/2018LF05.csv'\n",
    "data = pd.read_csv(txt)\n",
    "\n",
    "tsample = data.jd\n",
    "fsample = data.magpsf\n",
    "flux_err = data.sigmapsf\n",
    "data_pts = len(tsample)\n",
    "\n",
    "tsample.iloc[-1]\n",
    "\n",
    "time = pd.Series(np.linspace(tsample.iloc[0], tsample.iloc[-1], 1000))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(8,4))\n",
    "ax.errorbar(tsample, fsample, yerr=flux_err, fmt = 'k.', label=\"Sample : \" + str(len(tsample)))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failed Hack Day Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure, show, gridplot, ColumnDataSource\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "data = sampler.chain.T.reshape(4*sampler.iterations,len(sampler.chain))\n",
    "df = pd.DataFrame(\n",
    "    data=data,\n",
    "    index=pd.MultiIndex.from_product([['mean', 'log_amp','gamma','log_p'], np.arange((sampler.iterations))], names=['parameter','steps']),\n",
    "    columns=np.arange(len(sampler.chain)))\n",
    "\n",
    "TOOLS = \"pan,wheel_zoom,box_zoom,reset,save,tap,box_select\"\n",
    "\n",
    "x_list_of_list = []\n",
    "for i in range(0,len(df.columns)):\n",
    "    x_list_of_list.append(df.index.levels[1])\n",
    "\n",
    "mean_list_of_list = df.xs('mean', level=0).values.T.tolist()\n",
    "p1 = figure(tools=TOOLS, width=350, plot_height=350, title=\"Trace Plot\")\n",
    "#p1.multi_line(x_list_of_list, mean_list_of_list)\n",
    "\n",
    "log_amp_list_of_list = df.xs('log_amp', level=0).values.T.tolist()\n",
    "p2 = figure(tools=TOOLS, width=350, plot_height=350, title=\"Trace Plot\")\n",
    "#p2.multi_line(x_list_of_list, log_amp_list_of_list)\n",
    "\n",
    "source = ColumnDataSource(df)\n",
    "\n",
    "for j in df.columns:\n",
    "    # need to repmat the name to be same dimension as index\n",
    "    name_for_display = np.tile(j, len(df.index.levels[1]))\n",
    "    source = ColumnDataSource({'x': df.index.levels[1].values, 'mean': df.xs('mean', level=0)[j].values,\n",
    "                               'log_amp': df.xs('log_amp', level=0)[j].values, 'chain': name_for_display})\n",
    "    \n",
    "    p1.circle('x', 'mean', source = source)\n",
    "    p2.circle('x', 'log_amp', source = source)\n",
    "    \n",
    "\n",
    "p = gridplot([[p1, p2]])\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../data/\"\n",
    "filename = \"phaethon_damit.txt\"\n",
    "data  = pd.read_csv(datadir+filename, header=None, delim_whitespace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      2.453344e+06\n",
       "1      2.453344e+06\n",
       "2      2.453344e+06\n",
       "3      2.453344e+06\n",
       "4      2.453344e+06\n",
       "5      2.453344e+06\n",
       "6      2.453344e+06\n",
       "7      2.453344e+06\n",
       "8      2.453344e+06\n",
       "9      2.453344e+06\n",
       "10     2.453344e+06\n",
       "11     2.453344e+06\n",
       "12     2.453344e+06\n",
       "13     2.453344e+06\n",
       "14     2.453344e+06\n",
       "15     2.453344e+06\n",
       "16     2.453344e+06\n",
       "17     2.453344e+06\n",
       "18     2.453344e+06\n",
       "19     2.453344e+06\n",
       "20     2.453344e+06\n",
       "21     2.453344e+06\n",
       "22     2.453344e+06\n",
       "23     2.453344e+06\n",
       "24     2.453344e+06\n",
       "25     2.453344e+06\n",
       "26     2.453344e+06\n",
       "27     2.453344e+06\n",
       "28     2.453344e+06\n",
       "29     2.453344e+06\n",
       "           ...     \n",
       "112    2.453345e+06\n",
       "113    2.453345e+06\n",
       "114    2.453345e+06\n",
       "115    2.453345e+06\n",
       "116    2.453345e+06\n",
       "117    2.453345e+06\n",
       "118    2.453345e+06\n",
       "119    2.453345e+06\n",
       "120    2.453345e+06\n",
       "121    2.453345e+06\n",
       "122    2.453345e+06\n",
       "123    2.453345e+06\n",
       "124    2.453345e+06\n",
       "125    2.453345e+06\n",
       "126    2.453345e+06\n",
       "127    2.453345e+06\n",
       "128    2.453345e+06\n",
       "129    2.453345e+06\n",
       "130    2.453345e+06\n",
       "131    2.453345e+06\n",
       "132    2.453345e+06\n",
       "133    2.453345e+06\n",
       "134    2.453345e+06\n",
       "135    2.453345e+06\n",
       "136    2.453345e+06\n",
       "137    2.453345e+06\n",
       "138    2.453345e+06\n",
       "139    2.453345e+06\n",
       "140    2.453345e+06\n",
       "141    2.453345e+06\n",
       "Name: 0, Length: 142, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsample = data[0]\n",
    "tsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
